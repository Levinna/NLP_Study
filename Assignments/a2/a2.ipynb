{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "0aa23quxnmGC",
        "outputId": "1f58a6ad-b5e4-433b-d2f3-d86c0b31c100"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from utils.gradcheck import gradcheck_naive\n",
        "from utils.utils import normalizeRows, softmax\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid function for the input here.\n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array.\n",
        "    Return:\n",
        "    s -- sigmoid(x)\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    s = 1 / (1 + np.exp(-x))\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "def naiveSoftmaxLossAndGradient(\n",
        "    centerWordVec,\n",
        "    outsideWordIdx,\n",
        "    outsideVectors,\n",
        "    dataset\n",
        "):\n",
        "    \"\"\" Naive Softmax loss & gradient function for word2vec models\n",
        "    Implement the naive softmax loss and gradients between a center word's \n",
        "    embedding and an outside word's embedding. This will be the building block\n",
        "    for our word2vec models.\n",
        "    Arguments:\n",
        "    centerWordVec -- numpy ndarray, center word's embedding\n",
        "                    (v_c in the pdf handout)\n",
        "    outsideWordIdx -- integer, the index of the outside word\n",
        "                    (o of u_o in the pdf handout)\n",
        "    outsideVectors -- outside vectors (rows of matrix) for all words in vocab\n",
        "                      (U in the pdf handout)\n",
        "    dataset -- needed for negative sampling, unused here.\n",
        "    Return:\n",
        "    loss -- naive softmax loss\n",
        "    gradCenterVec -- the gradient with respect to the center word vector\n",
        "                     (dJ / dv_c in the pdf handout)\n",
        "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
        "                    (dJ / dU)\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    ### Please use the provided softmax function (imported earlier in this file)\n",
        "    ### This numerically stable implementation helps you avoid issues pertaining\n",
        "    ### to integer overflow. \n",
        "\n",
        "    #这里的outsideVectors不同于pdf里的U，pdf里的U每列是一个词向量，这里每行是一个词向量\n",
        "    #理由是test_word2vec中，dummy_vectors的shpae是(10,3)，一共五个词，前五行是V矩阵，后五行是U矩阵\n",
        "    #所以outsideVectors的shape是(V,N),V既是词向量矩阵，也是词表大小。N是词向量维度。\n",
        "    #centerWordVec是行向量\n",
        "    logits = np.matmul(outsideVectors, np.transpose(centerWordVec)) #shpae (V,)\n",
        "    y_bar  = softmax(logits)\n",
        "    yo_bar = y_bar[outsideWordIdx]\n",
        "    loss = -np.log(yo_bar)\n",
        "\n",
        "    V,N = outsideVectors.shape\n",
        "    y = np.zeros(V)\n",
        "    y[outsideWordIdx] = 1\n",
        "    \n",
        "    gradCenterVec = np.matmul(y_bar - y, outsideVectors) # shape(N,),必须与v_c一致\n",
        "    gradOutsideVecs = np.outer(y_bar-y, centerWordVec) # shape(V,N),必须与U一致\n",
        "\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return loss, gradCenterVec, gradOutsideVecs\n",
        "\n",
        "\n",
        "def getNegativeSamples(outsideWordIdx, dataset, K):\n",
        "    \"\"\" Samples K indexes which are not the outsideWordIdx \"\"\"\n",
        "\n",
        "    negSampleWordIndices = [None] * K\n",
        "    for k in range(K):\n",
        "        newidx = dataset.sampleTokenIdx()\n",
        "        while newidx == outsideWordIdx:\n",
        "            newidx = dataset.sampleTokenIdx()\n",
        "        negSampleWordIndices[k] = newidx\n",
        "    return negSampleWordIndices\n",
        "\n",
        "\n",
        "def negSamplingLossAndGradient(\n",
        "    centerWordVec,\n",
        "    outsideWordIdx,\n",
        "    outsideVectors,\n",
        "    dataset,\n",
        "    K=10\n",
        "):\n",
        "    \"\"\" Negative sampling loss function for word2vec models\n",
        "    Implement the negative sampling loss and gradients for a centerWordVec\n",
        "    and a outsideWordIdx word vector as a building block for word2vec\n",
        "    models. K is the number of negative samples to take.\n",
        "    Note: The same word may be negatively sampled multiple times. For\n",
        "    example if an outside word is sampled twice, you shall have to\n",
        "    double count the gradient with respect to this word. Thrice if\n",
        "    it was sampled three times, and so forth.\n",
        "    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient\n",
        "    \"\"\"\n",
        "\n",
        "    # Negative sampling of words is done for you. Do not modify this if you\n",
        "    # wish to match the autograder and receive points!\n",
        "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
        "    indices = [outsideWordIdx] + negSampleWordIndices\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    u_o = outsideVectors[outsideWordIdx] \n",
        " \n",
        "    loss1 = -np.log(sigmoid(np.dot(u_o,centerWordVec)))\n",
        "    loss2 = -sum([np.log(sigmoid(-np.dot(outsideVectors[k],centerWordVec))) for k in negSampleWordIndices])\n",
        "    loss = loss1+loss2\n",
        "\n",
        "    score = np.dot(u_o,centerWordVec)\n",
        "    gradCenterVec = (sigmoid(score)-1)*u_o #shape (N,)\n",
        "    grad_u_o = (sigmoid(score)-1)*centerWordVec #shape (N,)\n",
        "    gradOutsideVecs = np.zeros(outsideVectors.shape)\n",
        "    gradOutsideVecs[outsideWordIdx] = grad_u_o \n",
        "    for k in negSampleWordIndices:\n",
        "        score = np.dot(outsideVectors[k],centerWordVec)\n",
        "        gradOutsideVecs[k] = gradOutsideVecs[k] + (1-sigmoid(-score))*centerWordVec\n",
        "        gradCenterVec += (1-sigmoid(-score))*outsideVectors[k]\n",
        "\n",
        "\n",
        "    ### Please use your implementation of sigmoid in here.\n",
        "\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return loss, gradCenterVec, gradOutsideVecs\n",
        "\n",
        "\n",
        "def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
        "             centerWordVectors, outsideVectors, dataset,\n",
        "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
        "    \"\"\" Skip-gram model in word2vec\n",
        "    Implement the skip-gram model in this function.\n",
        "    Arguments:\n",
        "    currentCenterWord -- a string of the current center word\n",
        "    windowSize -- integer, context window size\n",
        "    outsideWords -- list of no more than 2*windowSize strings, the outside words\n",
        "    word2Ind -- a dictionary that maps words to their indices in\n",
        "              the word vector list\n",
        "    centerWordVectors -- center word vectors (as rows) for all words in vocab\n",
        "                        (V in pdf handout)\n",
        "    outsideVectors -- outside word vectors (as rows) for all words in vocab\n",
        "                    (U in pdf handout)\n",
        "    word2vecLossAndGradient -- the loss and gradient function for\n",
        "                               a prediction vector given the outsideWordIdx\n",
        "                               word vectors, could be one of the two\n",
        "                               loss functions you implemented above.\n",
        "    Return:\n",
        "    loss -- the loss function value for the skip-gram model\n",
        "            (J in the pdf handout)\n",
        "    gradCenterVecs -- the gradient with respect to the center word vectors\n",
        "            (dJ / dV in the pdf handout)\n",
        "    gradOutsideVectors -- the gradient with respect to the outside word vectors\n",
        "                        (dJ / dU in the pdf handout)\n",
        "    \"\"\"\n",
        "\n",
        "    loss = 0.0\n",
        "    gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
        "    gradOutsideVectors = np.zeros(outsideVectors.shape)\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    centerWordId = word2Ind[currentCenterWord]\n",
        "    centerWordVec = centerWordVectors[centerWordId]\n",
        "    for ow in outsideWords:\n",
        "        owi = word2Ind[ow]\n",
        "        l, gradc,grado = word2vecLossAndGradient(centerWordVec,owi,outsideVectors,dataset)\n",
        "        loss += l \n",
        "        gradCenterVecs[centerWordId] += gradc\n",
        "        gradOutsideVectors += grado\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return loss, gradCenterVecs, gradOutsideVectors\n",
        "\n",
        "#############################################\n",
        "# Testing functions below. DO NOT MODIFY!   #\n",
        "#############################################\n",
        "\n",
        "def word2vec_sgd_wrapper(word2vecModel, word2Ind, wordVectors, dataset, \n",
        "                         windowSize,\n",
        "                         word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
        "    batchsize = 50\n",
        "    loss = 0.0\n",
        "    grad = np.zeros(wordVectors.shape)\n",
        "    N = wordVectors.shape[0]\n",
        "    centerWordVectors = wordVectors[:int(N/2),:]\n",
        "    outsideVectors = wordVectors[int(N/2):,:]\n",
        "    for i in range(batchsize):\n",
        "        windowSize1 = random.randint(1, windowSize)\n",
        "        centerWord, context = dataset.getRandomContext(windowSize1)\n",
        "\n",
        "        c, gin, gout = word2vecModel(\n",
        "            centerWord, windowSize1, context, word2Ind, centerWordVectors,\n",
        "            outsideVectors, dataset, word2vecLossAndGradient\n",
        "        )\n",
        "        loss += c / batchsize\n",
        "        grad[:int(N/2), :] += gin / batchsize\n",
        "        grad[int(N/2):, :] += gout / batchsize\n",
        "\n",
        "    return loss, grad\n",
        "\n",
        "\n",
        "def test_word2vec():\n",
        "    \"\"\" Test the two word2vec implementations, before running on Stanford Sentiment Treebank \"\"\"\n",
        "    dataset = type('dummy', (), {})()\n",
        "    def dummySampleTokenIdx():\n",
        "        return random.randint(0, 4)\n",
        "\n",
        "    def getRandomContext(C):\n",
        "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
        "        return tokens[random.randint(0,4)], \\\n",
        "            [tokens[random.randint(0,4)] for i in range(2*C)]\n",
        "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
        "    dataset.getRandomContext = getRandomContext\n",
        "\n",
        "    random.seed(31415)\n",
        "    np.random.seed(9265)\n",
        "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
        "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
        "\n",
        "    print(\"==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\")\n",
        "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
        "        skipgram, dummy_tokens, vec, dataset, 5, naiveSoftmaxLossAndGradient),\n",
        "        dummy_vectors, \"naiveSoftmaxLossAndGradient Gradient\")\n",
        "\n",
        "    print(\"==== Gradient check for skip-gram with negSamplingLossAndGradient ====\")\n",
        "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
        "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingLossAndGradient),\n",
        "        dummy_vectors, \"negSamplingLossAndGradient Gradient\")\n",
        "\n",
        "    print(\"\\n=== Results ===\")\n",
        "    print (\"Skip-Gram with naiveSoftmaxLossAndGradient\")\n",
        "\n",
        "    print (\"Your Result:\")\n",
        "    print(\"Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\nGradient wrt Outside Vectors (dJ/dU):\\n {}\\n\".format(\n",
        "            *skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
        "                dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset) \n",
        "        )\n",
        "    )\n",
        "\n",
        "    print (\"Expected Result: Value should approximate these:\")\n",
        "    print(\"\"\"Loss: 11.16610900153398\n",
        "Gradient wrt Center Vectors (dJ/dV):\n",
        " [[ 0.          0.          0.        ]\n",
        " [ 0.          0.          0.        ]\n",
        " [-1.26947339 -1.36873189  2.45158957]\n",
        " [ 0.          0.          0.        ]\n",
        " [ 0.          0.          0.        ]]\n",
        "Gradient wrt Outside Vectors (dJ/dU):\n",
        " [[-0.41045956  0.18834851  1.43272264]\n",
        " [ 0.38202831 -0.17530219 -1.33348241]\n",
        " [ 0.07009355 -0.03216399 -0.24466386]\n",
        " [ 0.09472154 -0.04346509 -0.33062865]\n",
        " [-0.13638384  0.06258276  0.47605228]]\n",
        "    \"\"\")\n",
        "\n",
        "    print (\"Skip-Gram with negSamplingLossAndGradient\")   \n",
        "    print (\"Your Result:\")\n",
        "    print(\"Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\n Gradient wrt Outside Vectors (dJ/dU):\\n {}\\n\".format(\n",
        "        *skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5,:],\n",
        "            dummy_vectors[5:,:], dataset, negSamplingLossAndGradient)\n",
        "        )\n",
        "    )\n",
        "    print (\"Expected Result: Value should approximate these:\")\n",
        "    print(\"\"\"Loss: 16.15119285363322\n",
        "Gradient wrt Center Vectors (dJ/dV):\n",
        " [[ 0.          0.          0.        ]\n",
        " [ 0.          0.          0.        ]\n",
        " [-4.54650789 -1.85942252  0.76397441]\n",
        " [ 0.          0.          0.        ]\n",
        " [ 0.          0.          0.        ]]\n",
        " Gradient wrt Outside Vectors (dJ/dU):\n",
        " [[-0.69148188  0.31730185  2.41364029]\n",
        " [-0.22716495  0.10423969  0.79292674]\n",
        " [-0.45528438  0.20891737  1.58918512]\n",
        " [-0.31602611  0.14501561  1.10309954]\n",
        " [-0.80620296  0.36994417  2.81407799]]\n",
        "    \"\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_word2vec()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ca2da956f18d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradcheck\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgradcheck_naive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalizeRows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTOy9x8Ano8A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}